{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35901bf-1176-431f-9697-7accfdde5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch transformers PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2827e19-ad50-4c35-9602-748fd01c8d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n",
      "Keyword arguments {'truncation': True, 'max_length': 512} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 Embedding Shape: (1, 768)\n",
      "Chunk 2 Embedding Shape: (1, 768)\n",
      "Chunk 3 Embedding Shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 1. PDF-Text extrahieren\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# 2. Text in Chunks aufteilen (max 512 Tokens für BERT)\n",
    "def split_text_into_chunks(text, tokenizer, max_length=512):\n",
    "    sentences = sent_tokenize(text)  # Robuste Satzaufteilung\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_length = len(tokenizer.tokenize(sentence, truncation=True, max_length=max_length))\n",
    "        if current_length + tokenized_length <= max_length:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += tokenized_length\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = tokenized_length\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# 3. Tokenizer und Modell laden\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 4. Embeddings erzeugen\n",
    "def generate_embeddings(text_chunks, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS-Token-Embedding\n",
    "        embeddings.append(cls_embedding.detach().numpy())\n",
    "    return embeddings\n",
    "\n",
    "# 5. Workflow ausführen\n",
    "pdf_path = \"Der kleine Schmied.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "chunks = split_text_into_chunks(text, tokenizer)\n",
    "embeddings = generate_embeddings(chunks, tokenizer, model)\n",
    "\n",
    "# Optional: Ergebnisse ausgeben oder speichern\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Chunk {i + 1} Embedding Shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ae9e26-bc34-41ab-9991-5441a2c7199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Beispieldatenbank: SQLite\n",
    "import sqlite3\n",
    "\n",
    "# Verbindung erstellen\n",
    "conn = sqlite3.connect(\"embeddings.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabelle erstellen\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS embeddings (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    chunk TEXT,\n",
    "    embedding BLOB\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Embeddings speichern\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    embedding_blob = np.array(embedding).tobytes()  # Konvertiere Embedding in BLOB\n",
    "    cursor.execute(\"INSERT INTO embeddings (chunk, embedding) VALUES (?, ?)\", (chunk, embedding_blob))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c96be5d-c3c2-452f-a440-9c50aa28cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Verbindung zur SQLite-Datenbank\n",
    "conn = sqlite3.connect(\"embeddings.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Embeddings aus der Datenbank abrufen\n",
    "cursor.execute(\"SELECT id, embedding FROM embeddings\")\n",
    "data = cursor.fetchall()\n",
    "\n",
    "# Embeddings in numpy-Array konvertieren\n",
    "ids = []\n",
    "embeddings = []\n",
    "for row in data:\n",
    "    ids.append(row[0])\n",
    "    embeddings.append(np.frombuffer(row[1], dtype=np.float32))\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# FAISS-Index erstellen\n",
    "dimension = embeddings.shape[1]  # z. B. 768\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Index speichern (optional)\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f958c94-b085-4e20-917e-695d6c4759b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort von GPT-2: Das \n",
      "hätte den Werkstoff wertlos gemacht \n",
      "An Samstagen wurde die Schmiede aufgeräumt und gefegt Jede Zange, jeder Hammer oder \n",
      "andere Geräte mussten an ihren Platz, das war Gesetz Und jedes Teilchen, sei es auch noch so \n",
      "klein, wurde aufgehoben „Ihr werdet schon sehen, für was das noch gut ist.“, erklärte der \n",
      "Großvater \n",
      "In der Vorweihnachtszeit, brachten die Landwirte ihre Gerätschaften zum Überprüften, bevor sie \n",
      "in den Winterschlaf versetzt wurden Pflugschare, Eggen, Platten, Kreuzhacken, Maurerhämmer \n",
      "mussten geschärft und gerichtet werden Wichtig war eine sorgfältige Kennzeichnung, damit \n",
      "jedes Teil seinem Besitzer zugeordnet werden konnte \n",
      "Eines Tages sagte der alte Schmied: „ Nun schmieden wir für jeden von euch einen \n",
      "Nussknacker“ Die Jungs schauten sich ungläubig an und verfolgten jeden Handgriff ihres \n",
      "Großvaters, der als erstes den passenden Stahl aussuchte Der Nussknacker durfte sich nicht \n",
      "verbiegen, sollte aber auch nicht zu schwer werden Und schön aussehen sollte er auch Und \n",
      "siehe da, aus alten Zinken von Eggen entstand nach vielen Arbeitsschritten ein brauchbarer \n",
      "Nussknacker Stolz wurde das selbstgefertigte Werk den Familien vorgeführt \n",
      "Kaufen kann jeder, aber selbst machen, dass war und ist auch heute noch  en    en Welt \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import faiss\n",
    "import sqlite3\n",
    "\n",
    "# FAISS-Index laden\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Anfrage-Embedding mit BERT erstellen\n",
    "query = \"Wer ist der Protagonist der Geschichte?\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "query_embedding = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Ähnliche Einträge im FAISS-Index finden\n",
    "k = 5  # Anzahl der relevanten Ergebnisse\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Ergebnisse aus SQLite abrufen\n",
    "conn = sqlite3.connect(\"embeddings.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Holen aller IDs aus der Datenbank\n",
    "cursor.execute(\"SELECT id FROM embeddings\")\n",
    "ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "relevant_chunks = []\n",
    "for idx in indices[0]:\n",
    "    cursor.execute(\"SELECT chunk FROM embeddings WHERE id=?\", (ids[idx],))\n",
    "    chunk = cursor.fetchone()\n",
    "    if chunk:\n",
    "        relevant_chunks.append(chunk[0])\n",
    "\n",
    "# Kontext mit abgerufenen Chunks erstellen\n",
    "context = \"\\n\".join(relevant_chunks) + \"\\n\\nFrage: Wer ist der Protagonist der Geschichte?\\nAntwort:\"\n",
    "\n",
    "# GPT-2 laden\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Kontext auf maximale Token-Länge begrenzen\n",
    "input_ids = gpt2_tokenizer.encode(context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "# GPT-2 Antwort generieren\n",
    "output = gpt2_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=gpt2_tokenizer.eos_token_id  # Explizite Angabe\n",
    ")\n",
    "\n",
    "response = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Antwort von GPT-2:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2e5b87-e965-4b3b-bfd1-a1ab8e646fec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "Cannot operate on a closed cursor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m relevant_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m----> 7\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT chunk FROM embeddings WHERE id=?\u001b[39m\u001b[38;5;124m\"\u001b[39m, (ids[idx],))\n\u001b[0;32m      8\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchone()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "\u001b[1;31mProgrammingError\u001b[0m: Cannot operate on a closed cursor."
     ]
    }
   ],
   "source": [
    "# FAISS-Index durchsuchen\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Relevante Chunks aus SQLite abrufen\n",
    "relevant_chunks = []\n",
    "for idx in indices[0]:\n",
    "    cursor.execute(\"SELECT chunk FROM embeddings WHERE id=?\", (ids[idx],))\n",
    "    chunk = cursor.fetchone()\n",
    "    if chunk:\n",
    "        relevant_chunks.append(chunk[0])\n",
    "\n",
    "# BERT initialisieren\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT verwenden, um die präziseste Antwort zu finden\n",
    "best_answer = None\n",
    "best_score = float(\"-inf\")\n",
    "\n",
    "for chunk in relevant_chunks:\n",
    "    # Tokenisierung\n",
    "    inputs = tokenizer(\n",
    "        query,  # Frage\n",
    "        chunk,  # Kontext\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Vorhersage mit BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "    # Score berechnen\n",
    "    score = torch.max(start_scores) + torch.max(end_scores)\n",
    "\n",
    "    # Beste Antwort auswählen\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores)\n",
    "        best_answer = tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index + 1])\n",
    "\n",
    "# Ergebnis ausgeben\n",
    "print(\"Beste Antwort:\", best_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44b420-7aeb-450b-a708-e4d1ab82a543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
