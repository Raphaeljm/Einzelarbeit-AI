{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d84b945-0568-47dd-9067-ace9a341e93a",
   "metadata": {},
   "source": [
    "# Text einlesen, encoden und als Enbeddings in Datenbank speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2b7014-55fa-4575-baf4-4f6c98c42864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rapha\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d4a4c-fd27-4f56-8107-6f06f3f19f63",
   "metadata": {},
   "source": [
    "### PDF einlesen und in Textkette speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7757b2db-051d-445e-86a5-8392f9b22835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf_directory(directory_path):\n",
    "    text = \"\"\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(directory_path, file_name)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            doc.close()\n",
    "    return text\n",
    "\n",
    "# Verzeichnis mit PDFs\n",
    "directory_path = \"C:/Users/rapha/Documents/Datascience/5. Semester/Artificial Intelligence/Einzelarbeit/Data\"\n",
    "\n",
    "extracted_text = extract_text_from_pdf_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e188564c-3189-4d84-95bc-20051f145553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell f√ºr das Encoding bestimmen\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Text in kleinere Abschnitte aufteilen\n",
    "text_segments = extracted_text.split(\"\\n\")  \n",
    "\n",
    "# Transformer Modell verwenden um Textsegmente zu encoden\n",
    "embeddings = model.encode(text_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc80c6d-2658-4d61-8e53-8a0941d2cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings als Index in Datenbank schreiben\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2-Distanz\n",
    "index.add(np.array(embeddings))\n",
    "faiss.write_index(index, \"faiss_index.db\") \n",
    "\n",
    "# Mapping von Segment-IDs zum Text speichern\n",
    "segment_mapping = {i: text for i, text in enumerate(text_segments)}\n",
    "\n",
    "# Mapping als JSON speichern\n",
    "with open(\"segment_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(segment_mapping, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
